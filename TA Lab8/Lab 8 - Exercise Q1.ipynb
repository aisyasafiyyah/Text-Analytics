{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4759a8bf-390b-4842-a7f7-4a69692d1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abfcb40c-5fc5-4923-aa77-97d38390bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ammar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "535c464e-8bb3-444d-9f47-f7213cc89749",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba4319d-614f-4cd1-9195-0878e080b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            1\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " camp\n",
      " mountain\n",
      " hike\n",
      " enjoy\n",
      " video\n",
      " sport\n",
      " prefer\n",
      " game\n",
      " book\n",
      " read\n",
      "\n",
      "Cluster 1:\n",
      " love\n",
      " footbal\n",
      " weekend\n",
      " go\n",
      " music\n",
      " concert\n",
      " listen\n",
      " play\n",
      " sport\n",
      " camp\n",
      "\n",
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_dataset)\n",
    "\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()\n",
    "\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd85d37c-d834-4d94-91e5-0722204f9906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n",
      "Purity: 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ammar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    \n",
    "    tokens = [ps.stem(word) for word in tokens if word not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "preprocessed_dataset = [preprocess_text(doc) for doc in dataset]\n",
    "\n",
    "tokenized_dataset = [doc.split() for doc in preprocessed_dataset]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100,\n",
    "                           window=5, min_count=1, workers=4)\n",
    "\n",
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in \n",
    "                       word2vec_model.wv], axis=0) for doc in preprocessed_dataset])\n",
    "\n",
    "k = 2\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "\n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6df7102-913b-40b6-bb51-4d0f38574cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
